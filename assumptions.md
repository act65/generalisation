__Assumption 1.__ `Large` enough models

The deep models we are working with are the right capacity, or greater (aka `over-parameterised`). Otherwise the models must `generalise` (this is a different sense of generalise) just to fit their training data.

_This should be easy to verify (?), hmph maybe not. Could use bytes of params vs bytes of data as a proxy?_

<!-- There are a couple of subtleties to having a `large` enough model. Just having enough parameters doesnt necessarily help you. I.e. a linear model... But how these parameters interact and their non linearities change how easy it is to learn different types of target function. -->

__Assumption 2.__

* effects of optimisation error are independent of ...? The optimisation process is independent of

__Assumption 3.__

* ? there must be a few more.
