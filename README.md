Some notes on generalisation.

## References

* [A closer look at memorization in deep networks.](https://arxiv.org/pdf/1706.05394v1.pdf)
* [Train longer, generalize better: closing the generalization gap in large batch training of neural networks.](https://arxiv.org/pdf/1705.08741.pdf)
* [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://arxiv.org/pdf/1705.08292v1.pdf)
* [Early Stopping without a Validation Set.](https://arxiv.org/pdf/1703.09580v2.pdf)
* [Understanding deep learning requires rethinking generalisation.](https://arxiv.org/pdf/1611.03530v2.pdf)
* [Sharp Minima Can Generalize For Deep Nets](https://arxiv.org/pdf/1703.04933.pdf)
* [Entropy-SGD: Bias gradient descent into wide valleys. ](https://arxiv.org/pdf/1611.01838.pdf)
* [Theory of Deep Learning III: Generalization Properties of SGD.](https://dspace.mit.edu/bitstream/handle/1721.1/107841/CBMM-Memo-067.pdf?sequence=1)
* [Flat Minima.](http://doi.org/10.1162/neco.1997.9.1.1)
* [On large batch training for deep learning: generalisation gap and sharp minima](??)
* [In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning](http://arxiv.org/abs/1412.6614)
* [Train faster, generalize better: Stability of stochastic gradient descent.](http://arxiv.org/abs/1509.01240)


Sokolic, J., Giryes, R., Sapiro, G., & Rodrigues, M. R. D. (2016). Generalization Error of Invariant Classifiers. Retrieved from https://arxiv.org/pdf/1610.04574v2.pdf
Li, C. J., Li, L., Qian, J., & Liu, J.-G. (2017). Batch Size Matters: A Diffusion Approximation Framework on Nonconvex Stochastic Gradient Descent. Retrieved from https://arxiv.org/pdf/1705.07562.pdf
Sokoli, J., Giryes, R., Sapiro, G., & Rodrigues, M. R. D. (n.d.). Robust Large Margin Deep Neural Networks. Retrieved
from https://arxiv.org/pdf/1605.08254v3.pdf
Arora, S., Ge, R., Liang, Y., Ma, T., & Zhang, Y. (2017). Generalization and Equilibrium in Generative Adversarial Nets (GANs). Retrieved from https://arxiv.org/pdf/1703.00573.pdf
