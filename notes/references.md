Most important references, as voted by me.

##### [Understanding deep learning requires rethinking generalisation.](https://arxiv.org/pdf/1611.03530v2.pdf)


##### [A closer look at memorization in deep networks.](https://arxiv.org/pdf/1706.05394v1.pdf)

##### [Deep learning is robust to massive label noise](https://arxiv.org/abs/1705.10694)

> We show that even without explicit cleaning or noise-robust algorithms, neural networks can learn from data that has been diluted by an arbitrary amount of label noise.

But what is the cost?

##### [Flat Minima.](http://doi.org/10.1162/neco.1997.9.1.1)

##### [Sharp minima can generalize for deep nets](https://arxiv.org/pdf/1703.04933.pdf)

Sharp minima (of NNs wrt their parameters) are equivalent (under the right definitions of flat and sharp) to flat minima as we can arbitrarily rescale the curvature of the loss.

They do not show that sharp minima can `generalise`. Only that under common definitions of flatness, functions with equivalent generalisation can be made to have arbitrarily flat or sharp minima.

##### [Entropy-SGD: Biasing gradient descent into wide valleys.](https://arxiv.org/abs/1611.01838)

How is the local entropy related to curvature?

##### [Theory of Deep Learning III: Generalization Properties of SGD.](https://dspace.mit.edu/bitstream/handle/1721.1/107841/CBMM-Memo-067.pdf?sequence=1)





##### [On large batch training for deep learning: generalisation gap and sharp minima](??)


##### [In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning](http://arxiv.org/abs/1412.6614)




##### [Robust Large Margin Deep Neural Networks.](https://arxiv.org/abs/1605.08254)

##### [Generalization Error of Invariant Classifiers.](https://arxiv.org/abs/1610.04574)


##### [Batch Size Matters: A Diffusion Approximation Framework on Nonconvex Stochastic Gradient Descent](https://arxiv.org/pdf/1705.07562.pdf)


##### [Train faster, generalize better: Stability of stochastic gradient descent.](http://arxiv.org/abs/1509.01240)

##### [Train longer, generalize better: closing the generalization gap in large batch training of neural networks.](https://arxiv.org/pdf/1705.08741.pdf)


##### [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://arxiv.org/pdf/1705.08292v1.pdf)


##### [Early Stopping without a Validation Set.](https://arxiv.org/pdf/1703.09580v2.pdf)
