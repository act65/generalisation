## Open questions

> How much of generalisation depends on the coupling between the loss function and the metric we measure on?

>

What does a neural networks loss surface actually look like? Would be great to have visualisations of this... The distribution of fixed points, a vector field of stability, ?

Existence and density of sharp minimizers of the loss surface.

How is sample complexity tied to the number of steps an optimiser takes. We often conflate these two things when thinking about generalisation error. Would be nice to investigate.

<!-- So can I just take a super small subset of data, add my own noise, and pick out the patterns because we see them more often(ohh, this reminds me of info maximisation under noise!!). Main problem is that the data still needs to be representative of all patterns. -->

## Wish list

What do we care about in practice?

* The ability to train on large batches and get good generalisation
* ?

## Conclusions

?
