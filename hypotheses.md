Why does the combination of NNs and SGD generalise so well?
Why does ?

What are people claiming?

__Claim 1.__ Bouncing around with SGD

SGD works because it is noisy, it bounces around, and is more likely to bounce out of sharp minima than flat minima. Therefore SGD learns flat minima.
Therefore SGD generalises better.

__Claim 2.__ ???

?


What about random hacks and solutions?
